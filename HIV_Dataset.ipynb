{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p09yYTB7Kj61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021fcc27-4f2f-48d8-d325-1a4f4c424087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepchem\n",
            "  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepchem) (1.2.2)\n",
            "Collecting scipy<1.9 (from deepchem)\n",
            "  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit (from deepchem)\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->deepchem) (2022.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit->deepchem) (8.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepchem) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\n",
            "Installing collected packages: scipy, rdkit, deepchem\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "Successfully installed deepchem-2.7.1 rdkit-2023.3.1 scipy-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install deepchem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "-IlyJYp1K7yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60a872f-3e6f-49a5-ef00-a0f13328cc35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=0ebbb663538ee4e1cdd374d4b126f767a39631888b09303b4aec14d01e6ad1db\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Dataset, Data\n",
        "import numpy as np \n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import deepchem as dc\n",
        "from rdkit import Chem"
      ],
      "metadata": {
        "id": "DiU4OPkBK4Al",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361e05a9-6b0c-4c38-a031-742204d97c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
            "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyOwnDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
        "        super().__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'HIV.csv'\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'not implemented.pt'\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        pass\n",
        "       \n",
        "        \n",
        "\n",
        "    def process(self):\n",
        "      self.data = pd.read_csv(self.raw_paths[0])\n",
        "      for index,mol_obj in tqdm(self.data.iterrows()):\n",
        "        mol = Chem.MolFromSmiles(mol_obj[\"smiles\"])\n",
        "        #get node features\n",
        "        node_feat = self.get_node_features(mol)\n",
        "\n",
        "        #get edge \n",
        "        edge = self.get_edge_features(mol)\n",
        "\n",
        "        #get adjacency matrix\n",
        "        adj = self.get_adj_matrix(mol)\n",
        "\n",
        "        #get labels\n",
        "        label = self.get_labels(mol_obj[\"HIV_active\"])\n",
        "\n",
        "        data = Data(x = node_feat , edge_index = edge , y = label , smiles=mol_obj[\"smiles\"])\n",
        "        \n",
        "        torch.save(data, os.path.join(self.processed_dir, f'data_{index}.pt'))\n",
        "    \n",
        "    def get_node_features(self , mol):\n",
        "\n",
        "      all_node_feat = []\n",
        "\n",
        "      for atom in mol.GetAtoms():\n",
        "        node_feats = []\n",
        "        \n",
        "        # Atomic number\n",
        "        node_feats.append(atom.GetAtomicNum())\n",
        "        \n",
        "        # Atom degree\n",
        "        node_feats.append(atom.GetDegree())\n",
        "        \n",
        "        # Formal charge\n",
        "        node_feats.append(atom.GetFormalCharge())\n",
        "        \n",
        "        # Hybridization\n",
        "        node_feats.append(atom.GetHybridization())\n",
        "        \n",
        "        # Aromaticity\n",
        "        node_feats.append(atom.GetIsAromatic())\n",
        "        \n",
        "        # Total Num Hs\n",
        "        node_feats.append(atom.GetTotalNumHs())\n",
        "        \n",
        "        # Radical Electrons\n",
        "        node_feats.append(atom.GetNumRadicalElectrons())\n",
        "        \n",
        "        # In Ring\n",
        "        node_feats.append(atom.IsInRing())\n",
        "        \n",
        "        \n",
        "        # Chirality\n",
        "        node_feats.append(atom.GetChiralTag())\n",
        "    \n",
        "        all_node_feat.append(node_feats)\n",
        "      \n",
        "      all_node_feat = np.asarray(all_node_feat)\n",
        "      return torch.tensor(all_node_feat, dtype=torch.float)\n",
        "\n",
        "\n",
        "    def get_edge_features(self, mol):\n",
        "\n",
        "      all_edge_feats = []\n",
        "\n",
        "      for bond in mol.GetBonds():\n",
        "        edge_feats = []\n",
        "\n",
        "        #Bond Type\n",
        "        edge_feats.append(bond.GetBondTypeAsDouble())\n",
        "\n",
        "        # Rings\n",
        "        edge_feats.append(bond.IsInRing())\n",
        "\n",
        "        all_edge_feats += [edge_feats , edge_feats]\n",
        "      \n",
        "      all_edge_feats = np.asarray(all_edge_feats)\n",
        "      return torch.tensor(all_edge_feats , dtype = torch.int64)\n",
        "    \n",
        "    def get_adj_matrix(self, mol):\n",
        "\n",
        "      edge_indices = []\n",
        "      for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            edge_indices += [[i, j], [j, i]]\n",
        "\n",
        "      edge_indices = torch.tensor(edge_indices)\n",
        "      edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
        "      return edge_indices\n",
        "    \n",
        "    def get_labels(self, label):\n",
        "      label = np.asarray([label])\n",
        "      return torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "      data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt')) \n",
        "      return data"
      ],
      "metadata": {
        "id": "yoCUY3y3LaXr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyOwnDataset(root = \"data/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Zn1HSktIq7",
        "outputId": "be4bfa31-696a-40ad-c89e-7db714c65094"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "35133it [01:25, 418.32it/s][00:27:51] WARNING: not removing hydrogen atom without neighbors\n",
            "[00:27:51] WARNING: not removing hydrogen atom without neighbors\n",
            "41127it [01:40, 409.00it/s]\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv, TopKPooling \n",
        "from torch.nn import Linear"
      ],
      "metadata": {
        "id": "Q39fhIdQDD13"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import embedding\n",
        "\n",
        "class ClassificationHIV(torch.nn.Module):\n",
        "  def __init__(self , feature_size):\n",
        "    super(ClassificationHIV , self).__init__()\n",
        "    num_classes = 2\n",
        "    embedding_size = 1024\n",
        "\n",
        "    self.layer1 = torch.nn.Sequentail(GATConv(feature_size , embedding_size , heads = 4 , dropout = 0.3),\n",
        "                                      Linear(embedding_size*4 , embedding_size),\n",
        "                                      TopKPooling(embedding_size , ratio = 0.8))\n",
        "    \n",
        "    self.layer2 = torch.nn.Sequentail(GATConv(feature_size , embedding_size , heads = 4 , dropout = 0.3),\n",
        "                                      Linear(embedding_size*4 , embedding_size),\n",
        "                                      TopKPooling(embedding_size , ratio = 0.8))\n",
        "    \n",
        "    self.layer3 = torch.nn.Sequentail(GATConv(feature_size , embedding_size , heads = 4 , dropout = 0.3),\n",
        "                                      Linear(embedding_size*4 , embedding_size),\n",
        "                                      TopKPooling(embedding_size , ratio = 0.8))\n",
        "    \n",
        "    #Linear Layer\n",
        "    self.layer4 = Linear(embedding_size * 3 , 1024)\n",
        "    self.layer5 = Linear(1024 , num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hRjWAj7X1gOO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F \n",
        "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
        "from torch_geometric.nn import TransformerConv, TopKPooling \n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, feature_size, model_params):\n",
        "        super(GNN, self).__init__()\n",
        "        embedding_size = model_params[\"model_embedding_size\"]\n",
        "        n_heads = model_params[\"model_attention_heads\"]\n",
        "        self.n_layers = model_params[\"model_layers\"]\n",
        "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
        "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
        "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
        "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
        "        edge_dim = model_params[\"model_edge_dim\"]\n",
        "\n",
        "        self.conv_layers = ModuleList([])\n",
        "        self.transf_layers = ModuleList([])\n",
        "        self.pooling_layers = ModuleList([])\n",
        "        self.bn_layers = ModuleList([])\n",
        "\n",
        "        # Transformation layer\n",
        "        self.conv1 = TransformerConv(feature_size, \n",
        "                                    embedding_size, \n",
        "                                    heads=n_heads, \n",
        "                                    dropout=dropout_rate,\n",
        "                                    edge_dim=edge_dim,\n",
        "                                    beta=True) \n",
        "\n",
        "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
        "        self.bn1 = BatchNorm1d(embedding_size)\n",
        "\n",
        "        # Other layers\n",
        "        for i in range(self.n_layers):\n",
        "            self.conv_layers.append(TransformerConv(embedding_size, \n",
        "                                                    embedding_size, \n",
        "                                                    heads=n_heads, \n",
        "                                                    dropout=dropout_rate,\n",
        "                                                    edge_dim=edge_dim,\n",
        "                                                    beta=True))\n",
        "\n",
        "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
        "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
        "            if i % self.top_k_every_n == 0:\n",
        "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
        "            \n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
        "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))  \n",
        "        self.linear3 = Linear(int(dense_neurons/2), 1)  \n",
        "\n",
        "    def forward(self, x, edge_attr, edge_index, batch_index):\n",
        "        # Initial transformation\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = torch.relu(self.transf1(x))\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # Holds the intermediate graph representations\n",
        "        global_representation = []\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
        "            x = torch.relu(self.transf_layers[i](x))\n",
        "            x = self.bn_layers[i](x)\n",
        "            # Always aggregate last layer\n",
        "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
        "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](\n",
        "                    x, edge_index, edge_attr, batch_index\n",
        "                    )\n",
        "                # Add current representation\n",
        "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
        "    \n",
        "        x = sum(global_representation)\n",
        "\n",
        "        # Output block\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        x = F.dropout(x, p=0.8, training=self.training)\n",
        "        x = self.linear3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "3p7VHqiaM8bj"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}